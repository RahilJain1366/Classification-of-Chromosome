{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMbSvfwIFZCA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "2fe25b60-b8a5-4b0b-e8aa-d4c600bb1b44"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck2YbcOUFdUa"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation, Conv2D, MaxPooling2D\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.applications import ResNet50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9HNQWy1qKHL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi9LEOk2FyPO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "b8aa360b-3fa4-4195-be62-2f1c0d2d68b1"
      },
      "source": [
        "base_dir = \"/content/drive/My Drive/chromosome/\"\n",
        "img_width = 128\n",
        "img_height = 128\n",
        "batch_size = 2\n",
        "os.listdir(base_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3',\n",
              " '24',\n",
              " '23',\n",
              " '8',\n",
              " '7',\n",
              " '5',\n",
              " '4',\n",
              " '9',\n",
              " '6',\n",
              " '22',\n",
              " '13',\n",
              " '21',\n",
              " '19',\n",
              " '15',\n",
              " '16',\n",
              " '17',\n",
              " '2',\n",
              " '20',\n",
              " '14',\n",
              " '18',\n",
              " '11',\n",
              " '10',\n",
              " '12',\n",
              " '1']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MDfi1AFFzyk"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rescale =1. / 255,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    validation_split = 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otF_IxlqF6NM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a60d9288-5647-433b-c8e9-004987ac8e47"
      },
      "source": [
        "train_generator = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size = (img_width,img_height),\n",
        "    batch_size = batch_size,\n",
        "    subset=\"training\",\n",
        "    class_mode=\"categorical\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3842 images belonging to 24 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7YW4V_PAVRr"
      },
      "source": [
        "val_datagen = ImageDataGenerator(\n",
        "    rescale =1. / 255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1f7L6TxAVeo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3d9acfa-b1cd-463e-da4e-a294747a9480"
      },
      "source": [
        "val_generator = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size = (img_width,img_height),\n",
        "    batch_size = batch_size,\n",
        "    subset=\"validation\",\n",
        "    class_mode=\"categorical\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1632 images belonging to 24 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cvZWDQYAVnQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Woq800h6AVvo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2Kejxk0F8t8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa56518e-9b9d-4190-956c-316e18b45875"
      },
      "source": [
        "restnet = ResNet50(include_top=False, weights='imagenet', input_shape=(128,128,3))\n",
        "for layer in restnet.layers:\n",
        "  layer.trainable = False\n",
        "output = restnet.layers[-1].output\n",
        "output = tf.keras.layers.Flatten()(output)\n",
        "output = tf.keras.layers.Dense(512, activation='relu')(output)\n",
        "output = tf.keras.layers.Dropout(0.25)(output)\n",
        "output = tf.keras.layers.Dense(24, activation='softmax')(output)\n",
        "restnet = tf.keras.models.Model(restnet.input, outputs=output)\n",
        "\n",
        "\n",
        "\n",
        "restnet.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 134, 134, 3)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 64, 64, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 64, 64, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 64, 64, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 66, 66, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 32, 32, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 32, 32, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 32, 32, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 32, 32, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 32, 32, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 32, 32, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 32, 32, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 32, 32, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 32, 32, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 32, 32, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 32, 32, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 32, 32, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 32, 32, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 32, 32, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 32, 32, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 16, 16, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 16, 16, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 16, 16, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 16, 16, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 16, 16, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 16, 16, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 16, 16, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 16, 16, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 16, 16, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 16, 16, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 16, 16, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 16, 16, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 16, 16, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 16, 16, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 16, 16, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 16, 16, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 16, 16, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 16, 16, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 8, 8, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 8, 8, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 8, 8, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 8, 8, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 8, 8, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 8, 8, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 8, 8, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 8, 8, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 8, 8, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 8, 8, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 8, 8, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 8, 8, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 8, 8, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 8, 8, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 8, 8, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 8, 8, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 8, 8, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 8, 8, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 8, 8, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 8, 8, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 8, 8, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 8, 8, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 8, 8, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 8, 8, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 32768)        0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          16777728    flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 24)           12312       dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 40,377,752\n",
            "Trainable params: 16,790,040\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tdpXh2wGBPt"
      },
      "source": [
        "restnet.compile(loss = \"categorical_crossentropy\",\n",
        "             optimizer = \"adam\",\n",
        "             metrics = [\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVguIwr5_zNR"
      },
      "source": [
        "restnet.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = 1921,\n",
        "    epochs = 50,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps = 816)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uUYtUiIGGxV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9426441f-d357-433f-de41-dec4d820e4d5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 2.7898 - accuracy: 0.1966 - val_loss: 9.1251 - val_accuracy: 0.0482\n",
            "Epoch 2/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 2.3939 - accuracy: 0.2519 - val_loss: 6.5845 - val_accuracy: 0.0394\n",
            "Epoch 3/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.8232 - accuracy: 0.4183 - val_loss: 6.0864 - val_accuracy: 0.0394\n",
            "Epoch 4/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.6667 - accuracy: 0.4736 - val_loss: 6.7695 - val_accuracy: 0.0570\n",
            "Epoch 5/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.6533 - accuracy: 0.4758 - val_loss: 2.9347 - val_accuracy: 0.1388\n",
            "Epoch 6/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.7150 - accuracy: 0.4602 - val_loss: 2.3662 - val_accuracy: 0.3119\n",
            "Epoch 7/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.4363 - accuracy: 0.5523 - val_loss: 35.5254 - val_accuracy: 0.2199\n",
            "Epoch 8/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.3147 - accuracy: 0.5910 - val_loss: 5.6076 - val_accuracy: 0.1249\n",
            "Epoch 9/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.3538 - accuracy: 0.5708 - val_loss: 2.0606 - val_accuracy: 0.3535\n",
            "Epoch 10/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.2777 - accuracy: 0.6056 - val_loss: 1.5904 - val_accuracy: 0.5026\n",
            "Epoch 11/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.2279 - accuracy: 0.6117 - val_loss: 1.5083 - val_accuracy: 0.5551\n",
            "Epoch 12/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.0226 - accuracy: 0.6753 - val_loss: 1.0071 - val_accuracy: 0.6713\n",
            "Epoch 13/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.0856 - accuracy: 0.6672 - val_loss: 1.6030 - val_accuracy: 0.5289\n",
            "Epoch 14/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.0752 - accuracy: 0.6599 - val_loss: 1.2393 - val_accuracy: 0.6143\n",
            "Epoch 15/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.8501 - accuracy: 0.7284 - val_loss: 0.9813 - val_accuracy: 0.6991\n",
            "Epoch 16/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.8178 - accuracy: 0.7320 - val_loss: 1.0680 - val_accuracy: 0.6625\n",
            "Epoch 17/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.7432 - accuracy: 0.7547 - val_loss: 1.0511 - val_accuracy: 0.6866\n",
            "Epoch 18/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.7092 - accuracy: 0.7683 - val_loss: 1.2015 - val_accuracy: 0.6443\n",
            "Epoch 19/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.8346 - accuracy: 0.7318 - val_loss: 1.6224 - val_accuracy: 0.5588\n",
            "Epoch 20/100\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.7904 - accuracy: 0.7435 - val_loss: 1.1085 - val_accuracy: 0.6837\n",
            "Epoch 21/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.6829 - accuracy: 0.7739 - val_loss: 1.1853 - val_accuracy: 0.6881\n",
            "Epoch 22/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.6532 - accuracy: 0.7839 - val_loss: 29.3195 - val_accuracy: 0.6581\n",
            "Epoch 23/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.6339 - accuracy: 0.7937 - val_loss: 1.2359 - val_accuracy: 0.6603\n",
            "Epoch 24/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.9153 - accuracy: 0.7145 - val_loss: 1.7476 - val_accuracy: 0.4938\n",
            "Epoch 25/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.1217 - accuracy: 0.6448 - val_loss: 4.9656 - val_accuracy: 0.1015\n",
            "Epoch 26/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.5403 - accuracy: 0.5262 - val_loss: 2.0213 - val_accuracy: 0.4237\n",
            "Epoch 27/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.0574 - accuracy: 0.6572 - val_loss: 1.2779 - val_accuracy: 0.5968\n",
            "Epoch 28/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.8550 - accuracy: 0.7333 - val_loss: 1.0917 - val_accuracy: 0.6720\n",
            "Epoch 29/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.1343 - accuracy: 0.6400 - val_loss: 1.3050 - val_accuracy: 0.6012\n",
            "Epoch 30/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.7830 - accuracy: 0.7425 - val_loss: 1.3449 - val_accuracy: 0.6297\n",
            "Epoch 31/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.0100 - accuracy: 0.6782 - val_loss: 2.2670 - val_accuracy: 0.4127\n",
            "Epoch 32/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.9976 - accuracy: 0.6762 - val_loss: 1.4118 - val_accuracy: 0.5924\n",
            "Epoch 33/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.7214 - accuracy: 0.7695 - val_loss: 1.0898 - val_accuracy: 0.7093\n",
            "Epoch 34/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.7744 - accuracy: 0.7413 - val_loss: 1.2566 - val_accuracy: 0.6465\n",
            "Epoch 35/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.7744 - accuracy: 0.7515 - val_loss: 1.5010 - val_accuracy: 0.5880\n",
            "Epoch 36/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.7048 - accuracy: 0.7764 - val_loss: 1.1336 - val_accuracy: 0.7012\n",
            "Epoch 37/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.6150 - accuracy: 0.7981 - val_loss: 1.1205 - val_accuracy: 0.7188\n",
            "Epoch 38/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.7715 - accuracy: 0.7549 - val_loss: 1.3712 - val_accuracy: 0.6304\n",
            "Epoch 39/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.6397 - accuracy: 0.7893 - val_loss: 0.9040 - val_accuracy: 0.7509\n",
            "Epoch 40/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.6238 - accuracy: 0.7954 - val_loss: 1.3587 - val_accuracy: 0.6501\n",
            "Epoch 41/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.6448 - accuracy: 0.7946 - val_loss: 1.0538 - val_accuracy: 0.7137\n",
            "Epoch 42/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.7035 - accuracy: 0.7752 - val_loss: 1.1795 - val_accuracy: 0.6735\n",
            "Epoch 43/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.5251 - accuracy: 0.8280 - val_loss: 0.9408 - val_accuracy: 0.7370\n",
            "Epoch 44/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.8028 - accuracy: 0.7462 - val_loss: 2.9967 - val_accuracy: 0.2411\n",
            "Epoch 45/100\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.9577 - accuracy: 0.6938 - val_loss: 1.1819 - val_accuracy: 0.6654\n",
            "Epoch 46/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.5579 - accuracy: 0.8180 - val_loss: 1.0231 - val_accuracy: 0.7151\n",
            "Epoch 47/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.4868 - accuracy: 0.8402 - val_loss: 1.1895 - val_accuracy: 0.6932\n",
            "Epoch 48/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.4941 - accuracy: 0.8382 - val_loss: 1.1404 - val_accuracy: 0.6998\n",
            "Epoch 49/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.4903 - accuracy: 0.8395 - val_loss: 1.5376 - val_accuracy: 0.6421\n",
            "Epoch 50/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.7343 - accuracy: 0.7661 - val_loss: 2.1630 - val_accuracy: 0.4237\n",
            "Epoch 51/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.7749 - accuracy: 0.7457 - val_loss: 1.0829 - val_accuracy: 0.6947\n",
            "Epoch 52/100\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.6167 - accuracy: 0.7988 - val_loss: 1.1286 - val_accuracy: 0.7049\n",
            "Epoch 53/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.4936 - accuracy: 0.8387 - val_loss: 1.3150 - val_accuracy: 0.6735\n",
            "Epoch 54/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.7882 - accuracy: 0.7627 - val_loss: 2.0117 - val_accuracy: 0.5310\n",
            "Epoch 55/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.6179 - accuracy: 0.7961 - val_loss: 1.0720 - val_accuracy: 0.7341\n",
            "Epoch 56/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.6873 - accuracy: 0.7854 - val_loss: 1.7393 - val_accuracy: 0.5047\n",
            "Epoch 57/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.5841 - accuracy: 0.8068 - val_loss: 1.2449 - val_accuracy: 0.6822\n",
            "Epoch 58/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.5164 - accuracy: 0.8307 - val_loss: 1.2683 - val_accuracy: 0.6852\n",
            "Epoch 59/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.8741 - accuracy: 0.7177 - val_loss: 1.4304 - val_accuracy: 0.6019\n",
            "Epoch 60/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.8202 - accuracy: 0.7279 - val_loss: 1.1519 - val_accuracy: 0.6735\n",
            "Epoch 61/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.5853 - accuracy: 0.8017 - val_loss: 1.3717 - val_accuracy: 0.6684\n",
            "Epoch 62/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.4868 - accuracy: 0.8273 - val_loss: 1.3820 - val_accuracy: 0.6954\n",
            "Epoch 63/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.5107 - accuracy: 0.8290 - val_loss: 1.3901 - val_accuracy: 0.6764\n",
            "Epoch 64/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.5951 - accuracy: 0.8114 - val_loss: 10.4038 - val_accuracy: 0.0519\n",
            "Epoch 65/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.8539 - accuracy: 0.7315 - val_loss: 2.3845 - val_accuracy: 0.4076\n",
            "Epoch 66/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.8274 - accuracy: 0.7413 - val_loss: 2.5788 - val_accuracy: 0.4032\n",
            "Epoch 67/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.7474 - accuracy: 0.7520 - val_loss: 1.2655 - val_accuracy: 0.6698\n",
            "Epoch 68/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.5737 - accuracy: 0.8105 - val_loss: 1.3714 - val_accuracy: 0.6472\n",
            "Epoch 69/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.4797 - accuracy: 0.8429 - val_loss: 1.2664 - val_accuracy: 0.7100\n",
            "Epoch 70/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.6912 - accuracy: 0.7817 - val_loss: 1.2742 - val_accuracy: 0.6465\n",
            "Epoch 71/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.5334 - accuracy: 0.8261 - val_loss: 1.3215 - val_accuracy: 0.6691\n",
            "Epoch 72/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.5021 - accuracy: 0.8387 - val_loss: 1.1751 - val_accuracy: 0.7195\n",
            "Epoch 73/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.4105 - accuracy: 0.8675 - val_loss: 1.6768 - val_accuracy: 0.6545\n",
            "Epoch 74/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.4020 - accuracy: 0.8663 - val_loss: 1.1725 - val_accuracy: 0.7312\n",
            "Epoch 75/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.3958 - accuracy: 0.8709 - val_loss: 1.3779 - val_accuracy: 0.7137\n",
            "Epoch 76/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.4500 - accuracy: 0.8555 - val_loss: 1.4482 - val_accuracy: 0.6779\n",
            "Epoch 77/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.3432 - accuracy: 0.8857 - val_loss: 1.4886 - val_accuracy: 0.6969\n",
            "Epoch 78/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.4591 - accuracy: 0.8568 - val_loss: 3.0154 - val_accuracy: 0.4295\n",
            "Epoch 79/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.6517 - accuracy: 0.7905 - val_loss: 1.1231 - val_accuracy: 0.7027\n",
            "Epoch 80/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.3934 - accuracy: 0.8692 - val_loss: 1.1532 - val_accuracy: 0.7195\n",
            "Epoch 81/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.5306 - accuracy: 0.8278 - val_loss: 1.3759 - val_accuracy: 0.6947\n",
            "Epoch 82/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.3561 - accuracy: 0.8804 - val_loss: 1.1734 - val_accuracy: 0.7224\n",
            "Epoch 83/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.3212 - accuracy: 0.8923 - val_loss: 1.2786 - val_accuracy: 0.7392\n",
            "Epoch 84/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.3868 - accuracy: 0.8731 - val_loss: 3.2574 - val_accuracy: 0.4522\n",
            "Epoch 85/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.0824 - accuracy: 0.6624 - val_loss: 1.1903 - val_accuracy: 0.6998\n",
            "Epoch 86/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.4735 - accuracy: 0.8409 - val_loss: 1.2373 - val_accuracy: 0.7042\n",
            "Epoch 87/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.5054 - accuracy: 0.8387 - val_loss: 1.4854 - val_accuracy: 0.6698\n",
            "Epoch 88/100\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.7793 - accuracy: 0.7523 - val_loss: 1.3889 - val_accuracy: 0.6508\n",
            "Epoch 89/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.4839 - accuracy: 0.8451 - val_loss: 1.1795 - val_accuracy: 0.7027\n",
            "Epoch 90/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.3692 - accuracy: 0.8823 - val_loss: 1.2680 - val_accuracy: 0.7268\n",
            "Epoch 91/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.4118 - accuracy: 0.8643 - val_loss: 1.3268 - val_accuracy: 0.7078\n",
            "Epoch 92/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.3176 - accuracy: 0.8957 - val_loss: 1.3129 - val_accuracy: 0.7261\n",
            "Epoch 93/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.3461 - accuracy: 0.8872 - val_loss: 1.3827 - val_accuracy: 0.7202\n",
            "Epoch 94/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.3083 - accuracy: 0.8982 - val_loss: 1.3479 - val_accuracy: 0.7407\n",
            "Epoch 95/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.4814 - accuracy: 0.8477 - val_loss: 1.5006 - val_accuracy: 0.6479\n",
            "Epoch 96/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.5424 - accuracy: 0.8270 - val_loss: 1.2235 - val_accuracy: 0.7166\n",
            "Epoch 97/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.3937 - accuracy: 0.8677 - val_loss: 1.1810 - val_accuracy: 0.7327\n",
            "Epoch 98/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.3044 - accuracy: 0.8970 - val_loss: 1.3601 - val_accuracy: 0.7283\n",
            "Epoch 99/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.3779 - accuracy: 0.8770 - val_loss: 1.3798 - val_accuracy: 0.7049\n",
            "Epoch 100/100\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.2939 - accuracy: 0.9038 - val_loss: 1.3043 - val_accuracy: 0.7334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f98658dc048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN0Q_SgLGKeV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}